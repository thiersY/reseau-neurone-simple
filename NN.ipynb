{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thiersY/reseau-neurone-simple/blob/main/NN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFzGeaFzAv0D"
      },
      "source": [
        "# Un réseau neuronal simple à partir de zéro avec PyTorch et Google Colab\n",
        "\n",
        "Dans ce tutoriel, nous implémentons un réseau de neurones simple à partir de zéro en utilisant PyTorch.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZ-4jcT2Av0S"
      },
      "source": [
        "## À propos\n",
        "\n",
        "Dans ce tutoriel, nous allons implémenter un réseau de neurones simple à partir de zéro en utilisant PyTorch. L'idée du tutoriel est de vous apprendre les bases de PyTorch et comment il peut être utilisé pour implémenter un réseau de neurones à partir de zéro. Je passerai en revue certaines des fonctionnalités et concepts de base disponibles dans PyTorch qui vous permettront de créer vos propres réseaux de neurones.\n",
        "\n",
        "Ce didacticiel suppose que vous avez une connaissance préalable du fonctionnement d'un réseau de neurones. Ne t'inquiète pas! Même si vous n'êtes pas si sûr, tout ira bien. Pour les utilisateurs avancés de PyTorch, ce didacticiel peut toujours servir de rappel. Ce tutoriel s'en inspire fortement [Mise en œuvre du réseau de neurones](https://repl.it/talk/announcements/Build-a-Neural-Network-in-Python/5457) codé uniquement en utilisant Numpy. En fait, j'ai essayé de réimplémenter le code en utilisant PyTorch à la place et j'ai ajouté mes propres intuitions et explications. Grâce à [Samay](https://repl.it/@shamdasani) pour son travail phénoménal, j'espère que cela en inspirera beaucoup d'autres comme moi.\n",
        "\n",
        "Le module `torch` fournit tous les opérateurs **tensor** nécessaires dont vous aurez besoin pour implémenter votre premier réseau de neurones à partir de zéro dans PyTorch. C'est exact! Dans PyTorch, tout est un tenseur, c'est donc la première chose à laquelle vous devrez vous habituer. Importons les bibliothèques dont nous aurons besoin pour ce tutoriel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4lw3_4wlAv0U"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8vfnDw_Av0W"
      },
      "source": [
        "## Données\n",
        "Commençons par créer des exemples de données à l'aide de la commande `torch.tensor`. Dans Numpy, cela pourrait être fait avec `np.array`. Les deux fonctions ont le même objectif, mais dans PyTorch, tout est un tenseur par opposition à un vecteur ou à une matrice. Nous définissons les types dans PyTorch à l'aide de la commande `dtype=torch.xxx`.\n",
        "\n",
        "Dans les données ci-dessous, « X » représente le nombre d'heures étudiées et le temps que les étudiants ont passé à dormir, tandis que « y » représente les notes. La variable `xPredicted` est une entrée unique pour laquelle nous voulons prédire une note en utilisant les paramètres appris par le réseau de neurones. N'oubliez pas que le réseau de neurones veut apprendre un mappage entre « X » et « y », il essaiera donc de deviner à partir de ce qu'il a appris à partir des données de formation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "6b5Z_s7PAv0X"
      },
      "outputs": [],
      "source": [
        "X = torch.tensor(([2, 9], [1, 5], [3, 6]), dtype=torch.float) # 3 X 2 tensor\n",
        "y = torch.tensor(([92], [100], [89]), dtype=torch.float) # 3 X 1 tensor\n",
        "xPredicted = torch.tensor(([4, 8]), dtype=torch.float) # 1 X 2 tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIcR-IuXAv0X"
      },
      "source": [
        "Vous pouvez vérifier la taille des tenseurs que nous venons de créer avec la commande `size`. Cela équivaut à la commande `shape` utilisée dans des outils tels que Numpy et Tensorflow. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z7C1vE08Av0X",
        "outputId": "1aa0282b-9856-4910-886a-13013fa5ca20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 2])\n",
            "torch.Size([3, 1])\n"
          ]
        }
      ],
      "source": [
        "print(X.size())\n",
        "print(y.size())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fXNLFQfAv0Z"
      },
      "source": [
        "## Scaling\n",
        "\n",
        "Ci-dessous, nous effectuons une mise à l'échelle sur les données de l'échantillon. Notez que la fonction `max` renvoie à la fois un tenseur et les indices correspondants. Nous utilisons donc `_` pour capturer les indices que nous n'utiliserons pas ici car nous ne nous intéressons qu'aux valeurs maximales pour effectuer la mise à l'échelle. Parfait! Nos données sont maintenant dans un format très agréable que notre réseau de neurones appréciera plus tard."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aDh_YQd7Av0Z",
        "outputId": "42cb1e46-b0d7-40d2-a1c8-9a970a09c19c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.5000, 1.0000])\n"
          ]
        }
      ],
      "source": [
        "# scale units\n",
        "X_max, _ = torch.max(X, 0)\n",
        "xPredicted_max, _ = torch.max(xPredicted, 0)\n",
        "\n",
        "X = torch.div(X, X_max)\n",
        "xPredicted = torch.div(xPredicted, xPredicted_max)\n",
        "y = y / 100  # max test score is 100\n",
        "print(xPredicted)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6ABAr_1Av0Z"
      },
      "source": [
        "Notez qu'il y a deux fonctions `max` et `div` dont je n'ai pas parlé ci-dessus. Ils font exactement ce qu'ils impliquent : 'max' trouve la valeur maximale dans un vecteur... Je veux dire tenseur ; et `div` est fondamentalement une jolie petite fonction pour diviser deux tenseurs.\n",
        "\n",
        "## Modèle (graphique de calcul)\n",
        "Une fois les données traitées et au bon format, il ne vous reste plus qu'à définir votre modèle. C'est là que les choses commencent à changer un peu par rapport à la façon dont vous construiriez vos réseaux de neurones en utilisant, par exemple, quelque chose comme Keras ou Tensorflow. Cependant, vous vous rendrez compte rapidement au fur et à mesure que PyTorch ne diffère pas beaucoup des autres outils d'apprentissage en profondeur. En fin de compte, nous construisons un graphique de calcul, qui est utilisé pour dicter comment les données doivent circuler et quel type d'opérations sont effectuées sur ces informations.\n",
        "\n",
        "À des fins d'illustration, nous construisons le réseau de neurones ou le graphe de calcul suivant :\n",
        "\n",
        "\n",
        "![alt text](https://drive.google.com/uc?export=view&id=1l-sKpcCJCEUJV1BlAqcVAvLXLpYCInV6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "6eOoBz1iAv0a"
      },
      "outputs": [],
      "source": [
        "class Neural_Network(nn.Module):\n",
        "    def __init__(self, ):\n",
        "        super(Neural_Network, self).__init__()\n",
        "        # paramètres\n",
        "        # TODO: les paramètres peuvent être paramétrés au lieu de les déclarer ici\n",
        "        self.inputSize = 2\n",
        "        self.outputSize = 1\n",
        "        self.hiddenSize = 3\n",
        "        \n",
        "        # weights\n",
        "        self.W1 = torch.randn(self.inputSize, self.hiddenSize) # 3 X 2 tensor\n",
        "        self.W2 = torch.randn(self.hiddenSize, self.outputSize) # 3 X 1 tensor\n",
        "        \n",
        "    def forward(self, X):\n",
        "        self.z = torch.matmul(X, self.W1) # 3 X 3 \".dot\" ne diffuse pas dans PyTorch\n",
        "        self.z2 = self.sigmoid(self.z) # fonction d'activation\n",
        "        self.z3 = torch.matmul(self.z2, self.W2)\n",
        "        o = self.sigmoid(self.z3) # fonction d'activation final\n",
        "        return o\n",
        "        \n",
        "    def sigmoid(self, s):\n",
        "        return 1 / (1 + torch.exp(-s))\n",
        "    \n",
        "    def sigmoidPrime(self, s):\n",
        "        # dérivé de sigmoïde\n",
        "        return s * (1 - s)\n",
        "    \n",
        "    def backward(self, X, y, o):\n",
        "        self.o_error = y - o # erreur de sortie\n",
        "        self.o_delta = self.o_error * self.sigmoidPrime(o) # dérivée de sig à l'erreur\n",
        "        self.z2_error = torch.matmul(self.o_delta, torch.t(self.W2))\n",
        "        self.z2_delta = self.z2_error * self.sigmoidPrime(self.z2)\n",
        "        self.W1 += torch.matmul(torch.t(X), self.z2_delta)\n",
        "        self.W2 += torch.matmul(torch.t(self.z2), self.o_delta)\n",
        "        \n",
        "    def train(self, X, y):\n",
        "        # passe avant + arrière pour l'entraînement\n",
        "        o = self.forward(X)\n",
        "        self.backward(X, y, o)\n",
        "        \n",
        "    def saveWeights(self, model):\n",
        "        # nous utiliserons les fonctions de stockage interne de PyTorch\n",
        "        torch.save(model, \"NN\")\n",
        "        # vous pouvez recharger le modèle avec tous les poids et ainsi de suite avec :\n",
        "        # torch.load(\"NN\")\n",
        "        \n",
        "    def predict(self):\n",
        "        print (\"Predicted data based on trained weights: \")\n",
        "        print (\"Input (scaled): \\n\" + str(xPredicted))\n",
        "        print (\"Output: \\n\" + str(self.forward(xPredicted)))\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpHKZChLAv0b"
      },
      "source": [
        "Pour les besoins de ce tutoriel, nous n'allons pas parler de maths, c'est pour un autre jour. Je veux juste que vous ayez une idée de ce qu'il faut pour construire un réseau de neurones à partir de zéro en utilisant PyTorch. Décomposons le modèle qui a été déclaré via la classe ci-dessus.\n",
        "\n",
        "## En-tête de classe\n",
        "Tout d'abord, nous avons défini notre modèle via une classe car c'est la manière recommandée pour construire le graphe de calcul. L'en-tête de classe contient le nom de la classe `Neural Network` et le paramètre `nn.Module` qui indique essentiellement que nous définissons notre propre réseau de neurones.\n",
        "\n",
        "```python\n",
        "class Neural_Network(nn.Module):\n",
        "```\n",
        "\n",
        "## Initialisation\n",
        "L'étape suivante consiste à définir les initialisations ( `def __init__(self,)`) qui seront effectuées lors de la création d'une instance du réseau de neurones personnalisé. Vous pouvez déclarer les paramètres de votre modèle ici, mais généralement, vous déclarerez la structure de votre réseau dans cette section -- la taille des couches cachées, etc. Puisque nous construisons le réseau de neurones à partir de zéro, nous avons explicitement déclaré la taille des matrices de poids : une qui stocke les paramètres de l'entrée à la couche cachée ; et un qui stocke le paramètre de la couche masquée à la couche de sortie. Les deux matrices de poids sont initialisées avec des valeurs choisies au hasard dans une distribution normale via `torch.randn(...)`. Notez que nous n'utilisons pas de biais uniquement pour garder les choses aussi simples que possible.\n",
        "\n",
        "```python\n",
        "def __init__(self, ):\n",
        "    super(Neural_Network, self).__init__()\n",
        "    # paramètres\n",
        "    # TODO: les paramètres peuvent être paramétrés au lieu de les déclarer ici\n",
        "    self.inputSize = 2\n",
        "    self.outputSize = 1\n",
        "    self.hiddenSize = 3\n",
        "\n",
        "    # weights\n",
        "    self.W1 = torch.randn(self.inputSize, self.hiddenSize) # 3 X 2 tensor\n",
        "    self.W2 = torch.randn(self.hiddenSize, self.outputSize) # 3 X 1 tensor\n",
        "```\n",
        "\n",
        "## La fonction de transfert\n",
        "La fonction `forward` est l'endroit où toute la magie se produit (voir ci-dessous). C'est là que les données entrent et sont introduites dans le graphe de calcul (c'est-à-dire la structure de réseau neuronal que nous avons construite). Puisque nous construisons un réseau de neurones simple avec une couche cachée, notre fonction de transfert semble très simple :\n",
        "\n",
        "```python\n",
        "def forward(self, X):\n",
        "    self.z = torch.matmul(X, self.W1) \n",
        "    self.z2 = self.sigmoid(self.z) # fonction d'activation\n",
        "    self.z3 = torch.matmul(self.z2, self.W2)\n",
        "    o = self.sigmoid(self.z3) # fonction d'activation final\n",
        "    return o\n",
        "```\n",
        "\n",
        "La fonction `forward` ci-dessus prend l'entrée `X` et effectue ensuite une multiplication matricielle (`torch.matmul(...)`) avec la première matrice de poids `self.W1`. Ensuite, le résultat est appliqué une fonction d'activation, \"sigmoïde\". La matrice résultante de l'activation est ensuite multipliée par la deuxième matrice de poids \"self.W2\". Ensuite, une autre activation est effectuée, ce qui rend la sortie du réseau neuronal ou du graphe de calcul. Le processus que j'ai décrit ci-dessus est simplement ce qu'on appelle une \"passe anticipée\". Pour que les poids soient optimisés lors de l'entraînement, nous avons besoin d'un algorithme de rétropropagation.\n",
        "\n",
        "## La fonction arrière\n",
        "La fonction `backward` contient l'algorithme de rétropropagation, où le but est essentiellement de minimiser la perte par rapport à nos poids. En d'autres termes, les poids doivent être mis à jour de manière à ce que la perte diminue pendant que le réseau de neurones s'entraîne (enfin, c'est ce que nous espérons). Toute cette magie est possible avec l'algorithme de descente de gradient qui est déclaré dans la fonction `backward`. Prenez une minute ou deux pour inspecter ce qui se passe dans le code ci-dessous :\n",
        "\n",
        "```python\n",
        "def backward(self, X, y, o):\n",
        "    self.o_error = y - o # erreur de sortie\n",
        "    self.o_delta = self.o_error * self.sigmoidPrime(o) \n",
        "    self.z2_error = torch.matmul(self.o_delta, torch.t(self.W2))\n",
        "    self.z2_delta = self.z2_error * self.sigmoidPrime(self.z2)\n",
        "    self.W1 += torch.matmul(torch.t(X), self.z2_delta)\n",
        "    self.W2 += torch.matmul(torch.t(self.z2), self.o_delta)\n",
        "```\n",
        "\n",
        "Notez que nous effectuons de nombreuses multiplications de matrices avec les opérations de transposition via les opérations `torch.matmul(...)` et `torch.t(...)`, respectivement. Le reste est simplement une descente en gradient - il n'y a rien à faire."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVubohc_Av0c"
      },
      "source": [
        "## Entraînement\n",
        "Il ne reste plus qu'à former le réseau de neurones. Nous créons d'abord une instance du graphe de calcul que nous venons de construire :\n",
        "\n",
        "```python\n",
        "NN = Neural_Network()\n",
        "```\n",
        "\n",
        "Ensuite, nous formons le modèle pour `1000` tours. Notez que dans PyTorch, `NN(X)` appelle automatiquement la fonction `forward`, il n'est donc pas nécessaire d'appeler explicitement `NN.forward(X)`.\n",
        "\n",
        "Après avoir obtenu la sortie prévue pour chaque cycle d'entraînement, nous calculons la perte, avec le code suivant :\n",
        "\n",
        "```python\n",
        "torch.mean((y - NN(X))**2).detach().item()\n",
        "```\n",
        "\n",
        "L'étape suivante consiste à démarrer l'entraînement (avant + arrière) via `NN.train(X, y)`. Après avoir entraîné le réseau de neurones, nous pouvons stocker le modèle et générer la valeur prédite de l'instance unique que nous avons déclarée au début, \"xPredicted\".\n",
        "\n",
        "Entraînons-nous!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eVsnneK6Av0e",
        "outputId": "7c574cd1-4514-4799-b99c-24cd706170b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#0 Loss: 0.023130903020501137\n",
            "#100 Loss: 0.004318820778280497\n",
            "#200 Loss: 0.004086004104465246\n",
            "#300 Loss: 0.003987605217844248\n",
            "#400 Loss: 0.003900497453287244\n",
            "#500 Loss: 0.0038110651075839996\n",
            "#600 Loss: 0.003716830164194107\n",
            "#700 Loss: 0.0036171097308397293\n",
            "#800 Loss: 0.0035115480422973633\n",
            "#900 Loss: 0.0033999476581811905\n",
            "Predicted data based on trained weights: \n",
            "Input (scaled): \n",
            "tensor([0.5000, 1.0000])\n",
            "Output: \n",
            "tensor([0.9539])\n",
            "Finished training!\n"
          ]
        }
      ],
      "source": [
        "NN = Neural_Network()\n",
        "for i in range(1000):  # trains the NN 1,000 times\n",
        "    if (i % 100) == 0:\n",
        "        print (\"#\" + str(i) + \" Loss: \" + str(torch.mean((y - NN(X))**2).detach().item()))  # mean sum squared loss\n",
        "    NN.train(X, y)\n",
        "#NN.saveWeights(NN) # save weights\n",
        "\n",
        "NN.predict()\n",
        "\n",
        "print(\"Finished training!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umyoYcejAv0f"
      },
      "source": [
        "La perte ne cesse de diminuer, ce qui signifie que le réseau de neurones apprend quelque chose. C'est ça. Toutes nos félicitations! Vous venez d'apprendre à créer et à former un réseau de neurones à partir de zéro à l'aide de PyTorch. Il y a tellement de choses que vous pouvez faire avec le réseau peu profond que nous venons de mettre en place. Vous pouvez ajouter plus de couches cachées ou essayer d'incorporer les termes de biais pour la pratique. J'aimerais voir ce que vous allez construire à partir d'ici. Contactez-moi sur[Twitter](https://twitter.com/omarsar0) if you have any further questions or leave your comments here. Until next time!\n",
        "\n",
        "## References:\n",
        "- [PyTorch nn. Modules](https://pytorch.org/tutorials/beginner/pytorch_with_examples.html#pytorch-custom-nn-modules)\n",
        "- [Build a Neural Network with Numpy](https://enlight.nyc/neural-network)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.7.13 ('play')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "cf9800998463bc980d70cdbacff0c7e9a10687346dc898569e92f016d6e252c9"
      }
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}